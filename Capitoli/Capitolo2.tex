% !TEX encoding = UTF-8
% !TEX TS-program = pdflatex
% !TEX root = ../Tesi.tex
% !TEX spellcheck = it-IT

%************************************************

%************************************************
%Il sistema Url2vec si propone come alternativa alle metodologie di clustering di siti web basate sul contenuto testuale delle pagine~\cite{Rajaraman11}. 
I grafi possono essere utilizzati per modellare innumerevoli fenomeni reali, da quelli scientifici a quelli sociali. Questi rappresentano le entità non come singole unità descritte appieno dalle loro caratteristiche, ma piuttosto dai collegamenti che intercorrono tra di esse. Queste ''relazioni'' codificano la maggior parte delle informazioni latenti in un grafo, caratterizzandolo in maniera univoca.

In questa tesi si approfondiscono tecniche e metodologie per le estrazione di queste informazioni dal grafo del Web, più precisamente di un sito Web, indirizzate alla divisione delle pagine in cluster omogenei. Le pagine Web tuttavia non sono divisibili in classi ben definite, non ci sono informazioni esplicite che rappresentano univocamente i professori, gli studenti o i corsi. Data questa assenza, non è possibile generare un modello accurato di classificazione. Si è rivelato necessario risolvere il problema con un latro approccio.

A dispetto della diversità delle pagine Web nella rete, quelle che risiedono all'interno di una particolare organizzazione, spesso, condividono una certa struttura. Questa può essere appresa attraverso l'analisi del peso, della densità o della direzione degli hyperlink presenti nelle pagine Web, ovvero gli archi del grafo costruito su di esse. Ad esempio, il sito Web di un dipartimento di informatica conterrà pagine riguardanti i docenti, gli studenti, i corsi, la ricerca che saranno accessibili attraverso determinati \textit{percorsi} del grafo. L'analisi dei grafi comporta anche degli svantaggi, gli algoritmi della teoria dei grafi sono NP-completi \cite{Garey}, e questo non può essere trascurato considerando le dimensioni del Web. Nella metodologia presentata viene proposta l'alternativa di apprendere rappresentazioni vettoriali dei vertici all'interno del grafo, utilizzandole insieme alle informazioni estratte dal contenuto di una pagina Web, così da processare vettori linearmente indipendenti. Queste rappresentazioni vettoriali sono apprese attraverso la generazione di percorsi attraverso gli archi del grafo, creando delle sequenze di vertici che saranno trattate come frasi, dove le parole sono pagine Web. 


\subsection{NLP nel Web: URL Embedding}
In questo modo è possibile sfruttare i recenti sviluppi nel campo del Word Embedding. Questi algoritmi riescono a considerare sempre più fattori e restituire vettori sempre più accurati. Vengono incluse informazioni riguardanti il contesto di una parola, ovvero gli altri termini contenuti nella frase in cui compare la parola analizzata. Nel Web questo implica considerare le pagine ''più vicine'', ovvero le pagine che puntano o sono puntate direttamente dalla pagina in esame, privilegiando quelle che hanno più collegamenti con questa. Questa informazione riesce ad estrarre le informazioni latenti nella struttura del sito, ed a raggruppare le pagine similmente ad un algoritmo di partizionamento di un grafo. Queste informazioni hanno rivelato proprietà peculiari quando applicate su grandi testi. Il caso più noto e controverso è quello delle analogie. 

È stato osservato che correlazioni nascoste possono trovarsi nella differenza tra coppie di vettori~\cite{Mikolov13}, come nel caso di parole simili ma con leggere differenze come il genere o il numero, infatti esse appaiono in frasi tendenzialmente simili, ma con delle piccole differenze. In questo caso l'informazione può essere interpretata come il genere o il numero.
\\\\
Informazioni simili possono essere trovate anche nel campo del Web. È ormai consolidata la questione dell'autorità di alcune pagine~\cite{Kleinberg99} e di come queste abbiano un maggior numero di link in entrata. Alcune pagine tuttavia saranno accessibili prevalentemente attraverso alcuni percorsi e si troveranno quindi in un contesto con elementi simili. Il problema resta dare un senso a queste correlazioni ed un significato utilizzabile. Nello sviluppo di Url2vec, tuttavia, sono emerse corrispondenze abbastanza naturali, come le pagine dei professori con i relativi corsi insegnati o i laboratori di afferenza. 

Queste relazioni sono dovute al fatto che il grafo di un sito Web è fortemente connesso, ma la maggior parte delle volte in cui appare un determinato corso di studio appare anche il relativo docente e viceversa. 

\subsubsection{Differenza tra parole ed URL}
La sola analisi delle sequenze comunque può risultare limitata. Le pagine Web non sono parole, ad esse è associata un ulteriore informazione, ovvero il contenuto testuale. Questa proprietà si è rivelata molto utile, infatti combinando le informazioni ricavate dall'esame di tutti e due gli aspetti, il grado di accuratezza del processo di clustering è salito in modo significativo. Il contenuto di una pagina infatti fornisce informazioni fondamentali su di essa, ma queste possono variare significativamente anche tra pagine considerate dello stesso tipo, presentando ad esempio una diversa distribuzione dei termini. Analizzando la struttura connessa delle pagine Web, si è cercato costruire delle rappresentazioni più significative.
%https://www.ideals.illinois.edu/bitstream/handle/2142/45402/Timothy_Weninger.pdf?sequence=1}
%The parallel path framework for entity discovery on the Web
Saper sfruttare tale struttura può agevolare notevolmente i task di Web Mining. Esso infatti estrae conoscenza strutturata e facilmente processabile.

%A partire da questo punto stai per descrivere il COME hai realizzato il tuo sistema, peccato che manca il COSA, ossia cosa fa il tuo sistema, cosa hai fatto di interessante, in che modo il tuo sistema è capace di superare delle open challenges, ect.
Nelle sottosezioni che seguono si descrive il sistema realizzato, oltre che le tecniche e gli algoritmi utilizzati per la realizzazione degli obiettivi preposti.\\
Sono state realizzate tre macro componenti.
\begin{itemize}
\item \textit{Crawling delle pagine Web}. Questo è regolato da alcuni parametri che rendono il processo flessibile e altamente modificabile.
\item \textit{Costruzione del Dataset}, ovvero la strutturazione delle informazioni ottenute nel processo precedente, secondo alcuni criteri necessari per le successive elaborazioni.
\item \textit{Clustering delle pagine Web} attraverso la combinazione dell'analisi del contenuto testuale e delle relazioni tra le pagine.
\end{itemize}

\section{Web Crawling per l'estrazione dei dati}
\label{crawling}
Le proprietà che caratterizzano le pagine Web rendono complicato il processo di estrazione di informazioni, soprattutto nel caso in cui i contenuti vengono generati dinamicamente.

\begin{figure}[htb]
	\centering
	\includegraphics[width = 100mm]{crawlerarch.png}
	\caption{Architettura di un web crawler}
	\label{crawlerarch}
\end{figure}

Un Web crawler, chiamato anche web spider o web robot, è un componente software. I suoi obiettivi principali sono:
\begin{itemize}
\item raccogliere il più velocemente ed efficientemente possibile pagine utili, insieme alla struttura ad hyperlink che le collega;
\item copiare tutte le pagine che visita per elaborazioni future, per poi indicizzarle cosi che gli utenti possano trovarle più velocemente;
\item validare gli hyperlink e il codice HTML;
\item eseguire il Web Scraping delle pagine Web.
\end{itemize}

Esso esplora una pagina alla volta, analizandone la struttura e gli hyperlink contenuti. Questi sono immagazzinati nella ''frontiera'' che inizialmente vuota, conserva tutte le pagine ancora da esplorare. L'ordine di esplorazione e le politiche di filtraggio degli hyperlink possono variare in base al risultato desiderato. 

\begin{algorithm}[H]
\caption{Crawling BFS}
\begin{algorithmic}

	\State $url\gets homepage$;	\Comment{Pagina da cui iniziare}
	\State $queue\gets empty$ \Comment{Coda per la BFS}
	\State $analyzedVertex.add(url)$;	\Comment{Insieme di url già visitati}
	\State $maxDepth url$;	\Comment{Massima profondità di esplorazione}
 	
 	\vspace*{+0.5cm}
 	
	\State \textbf{Begin}
	\While{$queue \neq empty$} 
		\State $urlToAnalyze\gets queue.dequeue()$
		
		\If{$urlToAnalyze.depth \leq maxDepth$}
			\State $outlinks \gets urlToAnalyze.getOutlinks()$
		\Else
			\State $urlToAnalyze$
		\EndIf
		\If{$outlinks \neq null$}
			\State $outlinks \gets urlToAnalyze.getOutlinks()$
			\State $analyzedVertex.add(outlinks)$
			\State $serialize(urlToAnalyze)$
			\For{\textbf{each} link in outlinks}
				\State $serialize(link)$
				\State $queue.enqueue(link, urlToAnalyze + 1)$
			\EndFor
		\EndIf
		
	\EndWhile
	\State \textbf{End}
\end{algorithmic}
\end{algorithm}

\subsection{Proprietà del web crawler}
Nel processo di Crawling, data la natura non strutturata del Web, è necessaria l'applicazione di numerose tecniche e metodologie per un corretto funzionamento.

\subsubsection{Normalizzazione degli URL}
Il termine di normalizzazione, chiamato anche canonicalizzazione di URL, si riferisce al processo di modifica e standardizzazione di un URL in una maniera consistente, ad esempio alcuni siti Web mettono a disposizione gli stessi file o i medesimi contenuti attraverso URL differenti. 
\\\\
\texttt{http://domain.com/products/page.php?product=smartphone}
\\
\texttt{http://domain.com/products/smartphone.php}
\\\\
\texttt{http://www.domain.edu/courses}
\\
\texttt{http://www.domain.edu/courses/index.html}
\\\\
Le due coppie di URL nell’esempio puntano agli stessi contenuti. Altri casi sono URL che differscono solo per il protocollo (''\texttt{http://}'' o ''\texttt{https://}'') o l'omissione della stringa ''\texttt{www}''. Effettuando la normalizzazione, si sceglie un URL come formato di riferimento per accedere ad un determinato contenuto. Ci sono diversi tipi di normalizzazione che possono essere usati, come la conversione in minuscolo, rimuovere i ''.'' e ''..'' portando gli URL relativi ad URL assoluti, aggiungere slash finali al componente di percorso non vuoto.
\\\\
Una soluzione consiste nella creazione di un dizionario che ha come chiavi gli hashcode del contenuto testuale delle pagine e come valori una lista di tutti i diversi URL che hanno quel contenuto. In questo modo si riduce il problema ad una sola operazione così da annullare le relazioni e i vari di pattern da scovare nall'analisi degli URL per capire se sono la stessa pagina o meno. 

\subsubsection{Ricerca all'interno dello stesso dominio}
Per estrarre informazione e per il successivo processo di clustering delle pagine web, è necessario che le pagine estratte si riferiscano allo stesso dominio, o in altri termini, che appartengano allo stesso sito web.
\\
Questo viene effettuato per sfruttare la struttura gerarchica del dito web rivolto principalmente riuscire a  raccogliere le informazioni nascoste nel grafo e sopratutto negli hyperlink.

\subsubsection{Dimensione della ricerca}
Anche limitando la ricerca ad un solo dominio, la dimensione delle pagine da esplorare, e quindi la grandezza della frontiera, può aumentare considerevolmente o addirittura portare il processo di crawling a divergere.

\begin{figure}[htb]
	\centering
	\includegraphics[width = 100mm]{breadthdepth.png}
	\caption{Differenze tra ricerca in ampiezza e ricerca in profondità}
	\label{breadthsearch}
\end{figure}

È stata utilizzata una ricerca in ampiezza con un limite variabile di profondità. Ponendo un limite all'esplorazione del grafo delle pagine si garantisce la la terminazione del processo di crawling e utilizzando una ricerca in ampiezza si dà priorità alle pagine vicine al nodo radice (comunemente l'homepage). Questa scelta è stata dettata anche dal cercare di evitare le cosiddette ''spider traps''. Queste sono dei meccanismi utilizzati, intenzionalmente o involontariamente, dai server oggetto di crawling, che possono portare ad una generazione dinamica e potenzialmente infinita in URL univoci, e quindi considerati come pagine diverse. Questo può essere evitato non aggiungendo alla frontiera URL che contengono il carattere ''?'', ma questo non è sempre efficace.
\\\\
Inoltre è stato osservato che nei siti web entity-oriented, le informazioni ricercate si trovano quasi sempre nei primi livelli di gerarchia \cite{He13}.

\subsubsection{Restrizione dell'esplorazione}
La restrizione può essere effettuata sulle pagine da esplorare o dalla frequenza di richieste che è possibile effettuare.
\\\\
Il robots exclusion protocol è uno standard che consiste in un file (robots.txt), posto alla radice della gerarchia di un sito Web. In pratica, il file indica le regole utilizzate dai crawler per applicare restrizioni di analisi sulle pagine di un sito web. 
\\\\
Molte volte i crawler non sono ben accetti, in quanto possono rallentare pesantemente la navigazione. Se server effettua dei controlli sulle richieste ricevute e queste hanno una frequenza troppo elevata o seguono uno schema riconducibile ad una macchina queste possono essere ignorate. Può capitare che richieste continue e non curanti delle restrizioni imposte portino a bandire l'indirizzo IP del servizio trasgressore.\\
Per evitare tali conseguenze è opportuno seguire una certa etica nell'operazione di crawling.

\subsection{Estrazione delle liste}
\label{liste}
Riuscire a strutturare dati non strutturati può rivelarsi ostico. Molti tentativi, più o meno efficaci, sono stati effettuati a tale proposito.
\\
In questa tesi uno degli obiettivi è stato quello di prendere in considerazione i collegamenti all'interno delle pagine web, seguendo i percorsi generati dalla concatenazione di più hyperlink. Questa scelta è stata effettuata sulla base dei recenti progressi nel campo del natural language processing (NLP)~\cite{Turian10}. Esplorando la struttura del grafo del web, data la forte connessione che esiste fra i suoi nodi, estrarre informazione può rivelarsi un operazione tutt'altro che banale. 
\\\\
È stato utilizzato il concetto di \textbf{lista}. 
Per permettere una migliore visualizzazione dell'informazione descritta, quasi tutte le pagine Web vengono formattate utilizzando regole CSS. Di conseguenza, per poter effettuare correttamente questo task, bisogna prima elaborare la pagina Web con tutte le informazioni grafiche sui nodi HTML e solo successivamente si può procedere alla loro estrazione. Grazie alle informazioni ricavate dall’HTML della pagina Web e dalla posizione e dimensione dei singoli nodi, è possibile stabilire una struttura gerarchica ad albero dei nodi che la compongono. Questa struttura gerarchica ad albero permette di scoprire i record, ovvero dati strutturati (ad esempio provenienti da database) che sono allineati orizzontalmente, verticalmente e anche strutturalmente (elementi HTML ul, li, . . . ); permette quindi di individuare dei gruppi di record, ovvero delle liste di record.
\\
L’individuazione delle liste di raggiungere pagine che contengono elementi simili a quelli contenuti nella pagina che si sta visualizzando. 
In questo modo le pagine dovrebbero essere accessibili solo attraverso percorsi predeterminati e non in maniera fortemente connessa. I nodi e gli archi risultanti risultano così un sottoinsieme di quelli originali.

\section{Costruzione del Dataset}
I dati estratti nel processo vanno organizzati e ampliati in modo da garantire l'accesso e l'elaborazione in maniera agevole.
\\
Il processo di crawling restituisce il grafo delle pagine web e il contenuto testuale di ogni pagina esplorata, a queste va aggiunta la generazione delle sequenze attraverso la teoria dei \textit{Random Walk}, come spiegato in \ref{rwsection}. Inoltre per un minor spreco di risorse si è optato per la conversione degli URL in codici, ovvero una associazione univoca fra un URL ed un codice (e.g. un numero) molto più corto, così da risparmiare tempo di elaborazione e spazio di archiviazione.

\subsection{Random Walk}
\label{rwsection}
Per la generazione delle sequenze è stato utilizzata la teoria dei \textit{Random Walk}. In matematica, un Random Walk è la formalizzazione dell'idea di prendere passi successivi in direzioni casuali. Matematicamente parlando, è il processo stocastico più semplice, il processo markoviano. Qui sono utilizzati per ricavare percorsi pseudo casuali da attraversamento del grafo del web per ottenere sequenze di URL collegati semanticamente.
\\\\
In una passeggiata aleatoria monodimensionale si studia il moto di una particella puntiforme vincolata a muoversi lungo una retta nelle due direzioni consentite. Ad ogni movimento essa si sposta (a caso) di un passo a destra (con una probabilità fissata $\ p$) o a sinistra con una probabilità $\ 1-p$, ed ogni passo è di lunghezza uguale e indipendente dagli altri.
\begin{figure}[htb]
	\centering
	\includegraphics[width = 100mm]{randomwalkonedim.png}
	\caption{Rappresentazione visuale di 8 random walk monodimensionali. }
	\label{englishchinese}
\end{figure}
\\\\
In una passeggiata aleatoria bidimensionale si studia il moto di una particella vincolata a muoversi sul piano spostandosi casualmente ad ogni passo a destra o a sinistra con probabilità $\frac{1}{2}$, verso l'alto o verso il basso con probabilità $\ p=\frac{1}{2}$. In pratica ad ogni passo può compiere un movimento lungo una delle quattro diagonali con probabilità $\frac{1}{4}$. Ci si chiede con che probabilità la particella tornerà al punto di partenza.
\begin{figure}[htb]
	\centering
	\includegraphics[width = 100mm]{randomwalktwodim.png}
	\caption{Rappresentazione visuale di un random walk di 25.000 passi su due dimensioni. }
	\label{englishchinese}
\end{figure}
\\\\
Queste passeggiate aleatorie possono trovare effettivi riscontri in natura come il traiettoria percorso da una particella in un liquido o in un gas, il tragitto di un animale affamato o anche il prezzo di un titolo fluttuante o la situazione finanziaria di un giocatore d'azzardo. Tutti questi esempi possono essere espressi come random walk, anche se in natura potrebbero non essere veramente casuali.
\\\\
Un popolare modello di random walk è quello su un reticolo regolare, dove ad ogni passo si segue un determinato percorso basandosi su una qualche distribuzione di probabilità. Nel caso più semplice si può solo “saltare” sul sito vicino. In un semplice random walk simmetrico in un reticolo localmente finito, le probabilità di saltare su ognuno dei siti vicini è la stessa.
\\\\
Per definire un random walk formalmente, prese indipendenti variabili casuali $ Z_1, Z_2, Z_3, $ \dots dove ogni variabile è o 1 o -1, con una probabilita del 50\% per ognuno dei due casi, e dato 
\begin{equation}
  S_0 = 0
\end{equation}
\begin{equation}
  S_n = \sum_{j=1}^{n} Z_j
\end{equation}
  
la serie $\left \{ S_n \right \} $ è chiamata random walk semplice in $\mathbb{Z}$.
\\\\
Si consideri un attraversatore casuale della rete (random surfer) che, partendo da un pagina web, esegue un passo alla volta in questo modo: ad ogni iterazione, dalla pagina corrente, procede verso una pagina a caso tale che esiste un link che dalla pagina corrente punta a questa.
\\\\
Procedendo da pagina a pagina, visiterò alcuni nodi, più spesso di altri; intuitivamente, saranno nodi con molti link entranti da altri nodi frequentemente visitati. Ci sono alcuni problemi. Che succede se si arriva ad una pagina che non ha link in uscita? In questo caso è necessario introdurre un’altra operazione: il teletrasporto. In questa operazione l’attraversatore casuale salta dal nodo corrente ad un qualsiasi altro nodo sulla rete.
Se $N$ è il numero totale dei nodi nel grafo, l’operazione di teletrasporto porta l’attraversatore verso ogni nodo con probabilità $\frac{1}{N}$. Potrebbe anche trasportarsi sulla posizione corrente con probabilità di $\frac{1}{N}$. Questa operazione è chiamata quando si arriva ad un non senza nodi in uscita o con una probabilità$\ d$ data, con$\ 0 < d < 1$.


\subsection{Generazione delle sequenze}
Le sequenze rappresentano un percorso che un attraversatore casuale della rete seguirebbe cliccando su un hyperlink a caso fra tutti quelli disponibili nella pagina corrente (o eventualmente nelle liste). Questi percorsi, chiamati \textbf{Random Walk} (o passeggiate aleatorie), sono stati largamente utilizzati in molti algoritmi sui grafi e sul Web~\cite{aldous14} in quanto buone approssimazioni di comportamenti casuali. Il problema di questa tecnica applicata al Web si presenta quando l'attraversatore casuale arriva ad una pagina priva di hyperlink. La soluzione più diffusa consiste nell'effettuare un ''salto'' verso una qualsiasi altra pagina quando non ci sono outlink da seguire. 
\\\\
Qui il problema non si pone, in quanto le sequenze generate hanno una lunghezza fissata prima dell'esecuzione e se la generazione dovesse bloccarsi, la sequenza risultante sarà solo più piccola. Questa scelta è dovuta dal fatto che l'informazione cercata scaturisce da percorsi reali di navigazione e non necessita una lunghezza obbligatoria da rispettare, in quanto le sequenze possono essere viste come frasi di un testo, dove le parole sono gli URL.
\\\\
Per motivi di sperimentazione sono stati implementati tre tipi diversi di Random Walk, utilizzabili modificando i parametri di esecuzione dell'algoritmo.



\begin{algorithm}[H]
\caption{Generazione delle sequenze}
\begin{algorithmic}

	\State $numRandomWalks$;	\Comment{Numero di Random Walk da generare}
	\State $lengthRandomWalks$ \Comment{Lunghezza dei Random Walk}
 	
 	\vspace*{+0.5cm}
 	
	\State \textbf{Begin}
	\State $node \gets \Call{randomnode}{}$
	\For{$i \gets 0 \to numRandomWalks$} 
		\State $sequence.add(node)$
		
		\While{$sequence.length \leq lengthRandomWalks$}
			\If{$node.hasOutlinks()$}
				\State $node \gets node.getOutlinks(RandomIndex)$
				\State $sequence.add(node)$
			\Else
				\State $break$
			\EndIf
		\EndWhile
		\State $serialize(sequence)$
		
	\EndFor
	\State \textbf{End}
\end{algorithmic}
\end{algorithm}


\subsubsection{Random Walk standard}
Questo è il caso standard, ovvero si parte da un nodo casuale del grafo e si segue ogni volta un arco a caso fra quelli disponibili, fino al raggiungimento della lunghezza prefissata o all'impossibilità di proseguire.
\\
Da notare che questo processo e il precedente non sono completamente separati, in quanto la scelta di un nodo casuale e la consecutiva traiettoria, possono portare all'esplorazione di pagine non precedentemente visitate. È quindi necessario mantenere aggiornato il grafo immagazzinato.
\begin{figure}[htb]
	\centering
	\includegraphics[width = 100mm]{randomWalkongraph.png}
	\caption{Random walk sul grafo. }
	\label{englishchinese}
\end{figure}
\subsubsection{Random Walk con partenza fissa}
Qui l'unica differenza consiste nel punto di partenza del cammino. Infatti si può partire in un nodo prefissato del grafo (generalemnte l'homepage di un sto web), in modo da esplorare più percorsi possibili avente quel nodo come origine.

\subsubsection{Random Walk attraverso le Liste}
Qui invece si può seguire uno dei due approcci precedenti, ma con il vincolo delle liste, quindi limitando la camminata ad un sottoinsieme di quella precedente.

\subsection{Esempio di Dataset}
Di seguito sono elencati file generati nel processo di crawling e di generazione delle sequenze. Gli esempi riportati di seguito sono stati ottenuti analizzando il sito del dipartimento di informatica di Urbana, IL: \texttt{www.cs.illinois.edu}

\subsubsection{urlsMap.txt}
Contiene la associazioni fra gli URL e il relativo codice identificativo. Questo è dovuto dalle ragioni spiegate in precedenza, ovvero ridurre i tempi di elaborazione e spazio di archiviazione. 
\\\\
\texttt{
http://cs.illinois.edu,3\\
http://cs.illinois.edu/prospective-students,4\\
http://cs.illinois.edu/current-students,5\\
http://cs.illinois.edu/courses,6\\
http://cs.illinois.edu/alumni,7\\
http://cs.illinois.edu/research,8\\
http://cs.illinois.edu/news,9\\
http://cs.illinois.edu/partners,10\\
http://cs.illinois.edu/about-us,11\\
. . .\\
}
\subsubsection{vertex.txt}
Contiene il contenuto testuale di ogni pagina esplorata. Ogni riga è quindi formata da il codice identificativo di un URL e il relativo contenuto.
\\\\
\texttt{
1	department of computer science at illinois engineering at ...\\
2	prospective students department of computer science at ...\\
3	current students department of computer science at ...\\
4	courses department of computer science at illinois ...\\
5	alumni department of computer science at illinois ...\\
6	research department of computer science at illinois    ...\\
7	news department of computer science illinois engineering ...\\
8	partners department of computer science at illinois ...\\
9	about us department of computer science at illinois ...\\
. . .\\
}
\subsubsection{edges.txt}
Questo è il file principale per la generazione delle sequenze, qui sono immagazzinate tutte le relazioni fra le pagine, ovvero gli archi che le collegano. 
\\\\
\texttt{
1	1\\
1	2\\
1	3\\
1	4\\
1	5\\
1	6\\
1	7\\
1	8\\
1	9\\
. . .\\
}
\subsubsection{sequencesIDs.txt}
Contiene le sequenze generate. I codici relativi alle pagine web sono separati da '' -1 '' e la linea finisce con un '' -2 ''. Da notare che sono riportate le sequenze che partono da un nodo casuale del grafo.
\\\\
\texttt{
137 -1 2 -1 27 -1 8 -1 52 -1 53 -1 8 -1 8 -1 10 -1 13 -1 -2\\
506 -1 5 -1 14 -1 11 -1 6 -1 2 -1 27 -1 114 -1 111 -1 11 -1 -2\\
424 -1 4 -1 12 -1 6 -1 8 -1 53 -1 4 -1 7 -1 12 -1 8 -1 -2\\
616 -1 5 -1 6 -1 8 -1 8 -1 9 -1 1 -1 21 -1 6 -1 3 -1 -2\\
51 -1 7 -1 7 -1 38 -1 38 -1 25 -1 103 -1 27 -1 113 -1 12 -1 -2\\
429 -1 10 -1 3 -1 6 -1 4 -1 11 -1 8 -1 9 -1 9 -1 3 -1 -2\\
783 -1 421 -1 5 -1 9 -1 7 -1 5 -1 2 -1 8 -1 2 -1 24 -1 -2\\
506 -1 5 -1 8 -1 52 -1 53 -1 25 -1 40 -1 13 -1 11 -1 13 -1 -2\\
638 -1 63 -1 153 -1 62 -1 63 -1 152 -1 63 -1 155 -1 13 -1 7 -1 -2\\
. . .\\
}
\subsubsection{sequencesIDsFromHomepage.txt}
Contiene le sequenze generate che partono da uno stesso nodo. La generazione di questo file avviene esplicitando il nodo di origine di ogni sequenza nella fase di generazione.
\\\\
\texttt{
1 -1 8 -1 2 -1 14 -1 2 -1 2 -1 10 -1 14 -1 66 -1 3 -1 -2\\
1 -1 18 -1 39 -1 8 -1 8 -1 24 -1 1 -1 4 -1 7 -1 25 -1 -2\\
1 -1 23 -1 -2\\
1 -1 16 -1 10 -1 3 -1 29 -1 29 -1 4 -1 25 -1 97 -1 108 -1 -2\\
1 -1 20 -1 20 -1 1 -1 11 -1 3 -1 2 -1 9 -1 10 -1 13 -1 -2\\
1 -1 25 -1 48 -1 48 -1 44 -1 42 -1 4 -1 7 -1 38 -1 38 -1 -2\\
1 -1 25 -1 115 -1 4 -1 11 -1 11 -1 1 -1 2 -1 26 -1 13 -1 -2\\
1 -1 24 -1 4 -1 9 -1 60 -1 56 -1 6 -1 25 -1 113 -1 116 -1 -2\\
1 -1 21 -1 25 -1 135 -1 32 -1 13 -1 4 -1 25 -1 149 -1 59 -1 -2\\
. . .\\
}


\section{Web page Clustering}
%In questa sezione si parlerà della soluzione proposta come alternativa alle normali tecniche di clustering delle pagine Web basate esclusivamente sul contenuto testuale. Infatti il fulcro del sistema è basato su metodologie nate nel campo del \textbf{Natural Language Processing} ma applicate nel contesto Web, in modo da aggiungere
Il clustering delle pagine Web all'interno di uno stesso dominio può venire effettuato sfruttando la struttura connessa del sito o trattando le singole pagine Web come documenti. Nel primo caso si tratta principalmente di utilizzare algoritmi e tecniche derivanti dalla teoria dei grafi per partizionare il grafo, in modo da isolare i cluster di pagine ''simili''. Nel secondo caso invece viene analizzato unicamente il contenuto testuale visivo della pagina, si costruisce una collezione di documenti, un vocabolario dei termini e si considera ogni pagina indipendentemente dalle altre.

Gli hyperlink tra le pagine di uno stesso sito sono tipicamente utilizzati per organizzare i contenuti e puntano a contenuti inerenti ma non per forza simili. Mentre il contenuto testuale, data l'ambiguità del linguaggio naturale, può fornire indizi sbagliati e considerare diverse pagine correlate solo per una differente distribuzione dei termini.

In questa tesi si è scelto un approccio diverso, la rappresentazione di ogni pagina infatti viene creata unendo le informazioni sulla struttura della pagina all'interno del grafo con le informazioni sul contenuto testuale. È importante notare che non si applicano tecniche di partizionamento del grafo, le relazioni tra le pagine vengono apprese mediante i Random Walk generati nella fase precedente e codificate in uno spazio vettoriale. Questo comporta numerosi vantaggi computazionali. Inoltre in questo modo è possibile unire queste rappresentazioni con quelle basate sulla frequenza dei termini. In particolare le sequenze generate attraverso la teoria dei Random Walk, vengono trattate come frasi ed analizzate con tecniche di Word Embedding derivanti dal campo del Natural Language Processing. La rappresentazione vettoriale di ogni pagina non è umanamente comprensibile come la rappresentazione termini-documenti, ma è conveniente da elaborare. Queste sono state le premesse alla base del lavoro svolto, ovvero unire tecniche consolidate in aree specifiche e trasferire la conoscenza in altri domini applicativi.

L'algoritmo implementato prende in input l'insieme delle pagine, con il relativo contenuto, e le sequenze di Random Walk generate, restituendo rappresentazioni vettoriali per ogni pagina. La fase di apprendimento può comunque essere opportunamente personalizzata considerando maggiormente uno dei due aspetti o gli algoritmi utilizzati per ognuno. Questo è dovuto all'eterogeneità del Web. Infatti è stato notato che non tutti i siti tendono a formare cluster sferici, o ancora che l'organizzazione interna di un sito non sia sufficientemente informativa.